{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "DeepLearningCourse vol.2\n",
    "# ニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "パーセプトロンの復習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "パーセプトロンの活性化関数は、「ステップ関数」と呼ばれるものでした。この活性化関数を__シグモイド関数__と呼ばれる関数に変更することで、ニューラルネットワークになります。（シグモイド関数以外にも関数はあります。）\n",
    "\n",
    "### シグモイド関数\n",
    "ニューラルネットワークでよく用いられる活性化関数の一つは、以下で定義されるシグモイド関数です。\n",
    "\n",
    "$$sigmoid(z) = \\frac{1}{1+exp(-z)}$$\n",
    "\n",
    "パーセプトロンとニューラルネットワークの違いは、この活性化関数だけなのです。\n",
    "\n",
    "今後は、ステップ関数とシグモイド関数の違いについて見ていきましょう。\n",
    "\n",
    "![](https://s3.amazonaws.com/ai-standard/pic-d2-1.png)\n",
    "\n",
    "\n",
    "シグモイド関数はなめらかな曲線であり、__入力に対して連続的に出力が変化__します（0~1の数値）。\n",
    "\n",
    "一方、ステップ関数は0を境に急に出力を変えています。すなわち、シグモイド関数と違って０と１のどちらかしか出力をしません。\n",
    "\n",
    "\n",
    "２つの共通点は、ともに__非線形関数__だということです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### ReLU\n",
    "近年では、ReLU(Rectified Linear Unit)という関数が、活性化関数に用いられています。\n",
    "\n",
    "ReLUは入力が０を越えていれば、その入力をそのまま出力し、0以下ならば０を出力する関数です。\n",
    "\n",
    "$$ReLU(z) = \\begin{eqnarray}\n",
    "\\left\\{\n",
    "\\begin{array}{l}\n",
    "x\\ \\ (x > 0) \\\\\n",
    "0\\ \\ (x \\leq 0)\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{eqnarray}$$\n",
    "![](https://s3.amazonaws.com/ai-standard/relu.png)\n",
    "\n",
    "### 順伝播\n",
    "![](https://s3.amazonaws.com/ai-standard/pic-2d-3.png)\n",
    "\n",
    "\n",
    "ステップ関数ではない活性化関数を使ったパーセプトロンを以下の様に複数つなげたものを__ニューラルネットワーク__と呼びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![](https://s3.amazonaws.com/ai-standard/nn.png)\n",
    "\n",
    "ニューラルネットワークは，重み$w$やバイアス$b$を調整し、隠れ層のニューロンを増やすことでどんな関数でも近似することが出来ます。そして、層を深くすれば、より精度よく近似できるようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 損失関数\n",
    "### 2乗誤差\n",
    "損失関数として最も有名なのは__２乗誤差__（mean squared error:MSE）でしょう。これは以下の様に表されます。\n",
    "\n",
    "$$MSE = \\frac{1}{2}\\sum_{k}(y_k-t_k)^2$$\n",
    "\n",
    "$y_k$は入力データに対する出力、$t_k$は対応する正解データを示しています。全データに対して正解データと出力の値が近いほど損失関数が小さくなります。つまり、損失関数が小さくなるように学習させることで、より正解データに近い値を出力するネットワークが学習されます。\n",
    "\n",
    "### 交差エントロピー誤差\n",
    "分類問題の場合は、交差エントロピー誤差が用いられます。これは以下の様に表されます。\n",
    "\n",
    "$$E = - \\sum_{k} t_k log y_k$$\n",
    "\n",
    "ここで、logは底がeの自然対数を表します。$y_k$は入力データに対する出力、$t_k$は対応する正解ラベルを示しています。また、$t_k$は対応する正解ラベルとなるインデックスだけが１で、その他は０であるとします（one-hot表現）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### ニューラルネットワークの学習\n",
    "\n",
    "![](https://s3.amazonaws.com/ai-standard/pic-d2-4.png)\n",
    "\n",
    "誤差Eを０になるべく近づけるように、$w$や$b$を調整していきます。\n",
    "\n",
    "今回は直感的理解を得るために、以下のニューラルネットワークで学習の様子を勉強していくことにします。\n",
    "![](https://s3.amazonaws.com/ai-standard/pic-d2-5.png)\n",
    "\n",
    "パラメータの更新式は勾配降下法によって導くので以下の様に定義できます。\n",
    "\n",
    "$$w^1_k = w^1_k - \\alpha \\frac{\\partial E}{\\partial w^1_k}$$\n",
    "\n",
    "$$b_k = b_k - \\alpha \\frac{\\partial E}{\\partial b_k}$$\n",
    "\n",
    "$$w^2_k = w^2_k - \\alpha \\frac{\\partial E}{\\partial w^2_k}$$\n",
    "\n",
    "$$b = b_k - \\alpha \\frac{\\partial E}{\\partial b}$$\n",
    "\n",
    "パラメータの微分を求められれば、更新していくことがわかりました。\n",
    "では、どのようにして微分を求めればいいでしょうか？\n",
    "\n",
    "これからは、効率よく微分を計算する方法である__誤差逆伝播法__という考え方を勉強していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 誤差逆伝播法（Back Propagation）\n",
    "パラメータの更新式が得られたので、偏微分を計算していけば重みの学習ができるようになりました。\n",
    "\n",
    "しかし、機械にとって偏微分が残っていると計算上の都合がよくありません。ですから、偏微分を解析的に計算して、機械にとって最適な計算ができるようにする必要があります。\n",
    "\n",
    "まずは、偏微分を計算して、微分の項をなくしましょう。\n",
    "\n",
    "まずは、$w^2_k$の更新に関してです。\n",
    "\n",
    "$$w^2_k = w^2_k - \\alpha \\frac{\\partial E}{\\partial w^2_k}$$\n",
    "\n",
    "この式における、$\\frac{\\partial E}{\\partial w^2_k}$は、連鎖率を使うと\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w^2_k} = \\frac{\\partial E}{\\partial y} \\frac{\\partial y}{\\partial w^2_k}$$\n",
    "\n",
    "のようになります。\n",
    "\n",
    "![](https://s3.amazonaws.com/ai-standard/pic-d2-6.png)\n",
    "\n",
    "まずは、$\\frac{\\partial E}{\\partial y}$と$\\frac{\\partial y}{\\partial w^2_k}$を求めます。\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial y} = y-t$$\n",
    "\n",
    "#### シグモイド関数の微分\n",
    "$$\\sigma (x)' = (1-\\sigma(x))\\sigma(x)$$\n",
    "\n",
    "よって、\n",
    "$$\\frac{\\partial y}{\\partial w^2_k}　= (1-y)yu_k$$\n",
    "\n",
    "以上より、\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w^2_k} = \\frac{\\partial E}{\\partial y} \\frac{\\partial y}{\\partial w^2_k} = (y-t)(1-y)yu_k$$\n",
    "\n",
    "となります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "次に、bの更新に関してです。\n",
    "\n",
    "$$b = b_k - \\alpha \\frac{\\partial E}{\\partial b}$$\n",
    "\n",
    "$\\frac{\\partial E}{\\partial b}$この部分を同様に表します。\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial b}= \\frac{\\partial E}{\\partial y} \\frac{\\partial y}{\\partial b_k}$$\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial b_k} = (1-y)y$$\n",
    "\n",
    "以上より、\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial b}= \\frac{\\partial E}{\\partial y} \\frac{\\partial y}{\\partial b_k}=(y-t)(1-y)y$$\n",
    "\n",
    "となります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 中間層\n",
    "![](https://s3.amazonaws.com/ai-standard/pic-d2-7.png)\n",
    "$w^1_k$の更新についてです。\n",
    "$$w^1_k = w^1_k - \\alpha \\frac{\\partial E}{\\partial w^1_k}$$という更新をします。\n",
    "\n",
    "この偏微分で表された所は、連鎖率を使うと\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w^1_k} = \\frac{\\partial E}{\\partial y} \\frac{\\partial y}{\\partial u_k} \\frac{\\partial u_k}{\\partial w^1_k} =(y-t)(1-y)yw^2_k(1-u_k)u_kx$$\n",
    "\n",
    "となります。\n",
    "\n",
    "そして、$b_k$に関しても同様に、\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial b_k} = \\frac{\\partial E}{\\partial y} \\frac{\\partial y}{\\partial u_k} \\frac{\\partial u_k}{\\partial b_k} =(y-t)(1-y)yw^2_k(1-u_k)u_k$$\n",
    "\n",
    "となります。\n",
    "\n",
    "これで偏微分項すべてを、yやtなどので表すことができました。\n",
    "\n",
    "あとはこれを、コンピュータで何度も計算し、パラメータを修正していくだけです。\n",
    "\n",
    "\n",
    "以上が、__誤差逆伝播法__の簡単な例でした。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
